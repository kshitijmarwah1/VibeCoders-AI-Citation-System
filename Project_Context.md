 **Read this completely before modifying any code.  
 This project is production-oriented. Do NOT simplify logic or remove safeguards.**

* * *

## ğŸ”· PROJECT NAME (Working Title)

**Universal AI Hallucination & Citation Verification System**

* * *

## ğŸ”· HIGH-LEVEL GOAL

This project is a **generic, domain-aware, multi-input AI verification system** designed to:

* Detect hallucinations in AI-generated or human-written content
    
* Verify factual claims using real web sources
    
* Assign confidence scores
    
* Detect contradictions across sources
    
* Provide explainable, human-readable reasoning
    
* Work for **any user**, **any domain**, **any input format**
    
* Use **only pretrained, free, open-source models**
    
* Be **multi-user safe** (no per-user customization)
    

This is **NOT** a chatbot.  
This is **NOT** a fine-tuned per-user system.

This is a **universal verification engine**, similar in spirit to:

* VirusTotal (but for claims)
    
* Plagiarism detectors
    
* AI safety middleware
    

* * *

## ğŸ”· CORE DESIGN PRINCIPLES (DO NOT VIOLATE)

1. **Deterministic Core**
    
    * Verification decisions must come from deterministic logic:
        
        * similarity scores
            
        * credibility weighting
            
        * contradiction detection
            
    * LLMs must NEVER decide truth.
        
2. **LLMs Are Advisory Only**
    
    * LLMs are used ONLY for:
        
        * explanations
            
        * reasoning summaries
            
    * They must NOT:
        
        * override status
            
        * modify confidence
            
        * introduce new claims
            
3. **Single Unified Verification Pipeline**
    
    * All inputs (text, PDF, URL, DOCX, batch) must funnel into:
        
        ```
        normalized_text â†’ claim extraction â†’ verification
        ```
        
4. **Domain-Aware, Not User-Aware**
    
    * Domains (medical, finance, legal, etc.) influence thresholds and models
        
    * Users do NOT influence logic or configuration
        
5. **Pretrained-Only Mode**
    
    * No training data
        
    * No fine-tuning
        
    * All models are loaded from Hugging Face or similar public sources
        
    * Architecture is fine-tuning ready, but disabled by design
        

* * *

## ğŸ”· CURRENT PROJECT STAGE

### âœ… STATUS: **Production-Ready MVP**

The system already supports:

* Claim extraction & filtering
    
* Semantic similarity using Sentence Transformers
    
* Web grounding via search
    
* Credibility weighting by domain
    
* Contradiction detection
    
* Domain detection (ML-based + fallback)
    
* Overall reliability scoring
    
* Multi-format input support
    
* LLM-based explanations (non-authoritative)
    
* Clean service-based backend architecture
    

* * *

## ğŸ”· BACKEND ARCHITECTURE OVERVIEW

```
services/
â”‚
â”œâ”€â”€ api/                    # FastAPI layer (routes only)
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ routers/
â”‚       â””â”€â”€ verify.py
â”‚
â”œâ”€â”€ core/                   # ALL business logic lives here
â”‚   â”œâ”€â”€ input/              # Multi-format input adapters
â”‚   â”‚   â”œâ”€â”€ text.py
â”‚   â”‚   â”œâ”€â”€ pdf.py
â”‚   â”‚   â”œâ”€â”€ docx.py
â”‚   â”‚   â”œâ”€â”€ url.py
â”‚   â”‚   â”œâ”€â”€ batch.py
â”‚   â”‚   â””â”€â”€ normalize.py
â”‚   â”‚
â”‚   â”œâ”€â”€ claims/
â”‚   â”‚   â”œâ”€â”€ extractor.py          # Claim extraction + filtering
â”‚   â”‚   â”œâ”€â”€ domain_detector.py    # ML-based domain detection
â”‚   â”‚   â””â”€â”€ domain_classifier.py  # Pretrained classifier helper
â”‚   â”‚
â”‚   â”œâ”€â”€ verification/
â”‚   â”‚   â”œâ”€â”€ search.py             # Web search + snippets
â”‚   â”‚   â”œâ”€â”€ semantic.py           # Similarity computation
â”‚   â”‚   â”œâ”€â”€ contradiction.py      # Cross-source conflict detection
â”‚   â”‚   â”œâ”€â”€ model_registry.py     # Pretrained model loader
â”‚   â”‚   â””â”€â”€ verify.py             # CORE orchestration logic
â”‚   â”‚
â”‚   â”œâ”€â”€ scoring/
â”‚   â”‚   â”œâ”€â”€ domain.py             # Domain credibility weights
â”‚   â”‚   â”œâ”€â”€ credibility.py        # Credibility score
â”‚   â”‚   â””â”€â”€ aggregation.py        # Overall reliability score
â”‚   â”‚
â”‚   â”œâ”€â”€ explainability/
â”‚   â”‚   â””â”€â”€ traces.py             # Citation extraction
â”‚   â”‚
â”‚   â””â”€â”€ llm/
â”‚       â””â”€â”€ reasoner.py            # LLM explanation (advisory only)
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py               # Env vars
â”‚   â”œâ”€â”€ domain_loader.py          # YAML loader
â”‚   â””â”€â”€ domains/                  # Domain configs
â”‚       â”œâ”€â”€ general.yaml
â”‚       â”œâ”€â”€ medical.yaml
â”‚       â”œâ”€â”€ finance.yaml
â”‚       â”œâ”€â”€ legal.yaml
â”‚       â””â”€â”€ technology.yaml
â”‚
â””â”€â”€ storage/
    â””â”€â”€ cache.py                  # Global cache
```

* * *

## ğŸ”· VERIFICATION PIPELINE (STEP-BY-STEP)

**THIS IS THE MOST IMPORTANT PART â€” DO NOT BREAK THIS FLOW**

```
User Input (any format)
   â†“
Input Normalization (text/PDF/URL/DOCX)
   â†“
Claim Extraction
   â†“
Domain Detection (ML-based)
   â†“
For each claim:
   â”œâ”€ Web Search (sources + snippets)
   â”œâ”€ Semantic Similarity (domain model)
   â”œâ”€ Credibility Weighting (domain YAML)
   â”œâ”€ Contradiction Detection
   â”œâ”€ Final Confidence Score
   â”œâ”€ Status: verified / hallucinated
   â””â”€ LLM Explanation (optional, advisory)
   â†“
Overall Reliability Score
   â†“
Final Response
```

* * *

## ğŸ”· DOMAIN SYSTEM (VERY IMPORTANT)

Domains affect:

* Similarity thresholds
    
* Contradiction penalties
    
* Credibility weights
    
* Embedding model choice
    

Domains are defined in **YAML**, not code.

Example fields:

```yaml
similarity_threshold
contradiction_threshold
contradiction_penalty
trusted_domains
credibility_weights
```

DO NOT hardcode thresholds in Python.

* * *

## ğŸ”· LLM INTEGRATION RULES

* LLMs are ONLY used in `core/llm/reasoner.py`
    
* LLM output is **pure explanation**
    
* LLM must not:
    
    * change status
        
    * change confidence
        
    * add facts
        
* If LLM fails â†’ system still works
    

* * *

## ğŸ”· MULTI-USER SAFETY

* System is stateless per request
    
* No user sessions
    
* No personalization
    
* Cache is global and safe
    
* API is concurrency-safe
    

* * *

## ğŸ”· WHAT NOT TO DO (VERY IMPORTANT)

âŒ Do NOT:

* Merge verification + explanation logic
    
* Let LLM decide correctness
    
* Add user-specific configs
    
* Remove domain YAMLs
    
* Hardcode thresholds
    
* Collapse files back into one file
    

* * *

## ğŸ”· WHAT IS SAFE TO EXTEND

âœ… Safe future extensions:

* Frontend UX
    
* Async workers
    
* More input formats
    
* More pretrained models
    
* Deployment optimizations
    
* Better explanation prompts
    

* * *

## ğŸ”· EXPECTED OUTPUT STRUCTURE

Every verification response must include:

```json
{
  "domain": "...",
  "total_claims": 0,
  "overall_reliability": 0.0,
  "claims": [
    {
      "claim": "...",
      "status": "verified | hallucinated",
      "confidence": 0.0,
      "similarity": 0.0,
      "credibility": 0.0,
      "contradicted": false,
      "citations": [],
      "explanation": "..."
    }
  ]
}
```

* * *

## ğŸ”· FINAL INSTRUCTION TO AI ASSISTANT (IMPORTANT)

 When modifying this project:
 * Preserve architecture
 * Preserve verification logic  
 * Never simplify safety checks     
 * Ask before removing any module    
 * Treat this as a production system, not a demo
 